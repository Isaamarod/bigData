{"paragraphs":[{"text":"//Estadísticas simples de los datos: la media, la varianza el num de ceros por columna.\n","user":"anonymous","dateUpdated":"2019-05-18T13:03:12+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558177344709_-1992793375","id":"20190518-130224_1299182919","dateCreated":"2019-05-18T13:02:24+0200","dateStarted":"2019-05-18T13:02:33+0200","dateFinished":"2019-05-18T13:02:33+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:330"},{"text":"import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n\nval observations = sc.parallelize(\n  Seq(\n    Vectors.dense(1.0, 10.0, 100.0),\n    Vectors.dense(2.0, 20.0, 200.0),\n    Vectors.dense(3.0, 30.0, 300.0)\n  )\n)\n\n// Compute column summary statistics.\nval summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\nprintln(summary.mean)  // a dense vector containing the mean value for each column\nprintln(summary.variance)  // column-wise variance\nprintln(summary.numNonzeros)  // number of nonzeros in each column","user":"anonymous","dateUpdated":"2019-05-18T13:00:03+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[2.0,20.0,200.0]\n[1.0,100.0,10000.0]\n[3.0,3.0,3.0]\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nobservations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[0] at parallelize at <console>:28\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@5a298546\n"}]},"apps":[],"jobName":"paragraph_1558176779909_-1986741572","id":"20190518-125259_1732244802","dateCreated":"2019-05-18T12:52:59+0200","dateStarted":"2019-05-18T13:00:03+0200","dateFinished":"2019-05-18T13:00:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:331"},{"text":"//Estadísticas simples de los datos: número de elementos,mínimo, máximo.\n","user":"anonymous","dateUpdated":"2019-05-18T13:10:33+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558176785698_900178148","id":"20190518-125305_1434524244","dateCreated":"2019-05-18T12:53:05+0200","dateStarted":"2019-05-18T13:10:33+0200","dateFinished":"2019-05-18T13:10:33+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"text":"import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}\nval observations = sc.parallelize(\nSeq(\nVectors.dense(1.0, 10.0, 100.0),\nVectors.dense(2.0, 20.0, 200.0),\nVectors.dense(3.0, 30.0, 300.0)\n)\n)\n// Compute column summary statistics.\nval summary: MultivariateStatisticalSummary =Statistics.colStats(observations)\n// sample size\nprintln(summary.count)\n// Maximum value of each column\nprintln(summary.max)\n// Minimum value of each column\nprintln(summary.min)\n","user":"anonymous","dateUpdated":"2019-05-18T13:15:02+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"3\n[3.0,30.0,300.0]\n[1.0,10.0,100.0]\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nobservations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[2] at parallelize at <console>:32\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@334b13dc\n"}]},"apps":[],"jobName":"paragraph_1558177833543_488379967","id":"20190518-131033_720638154","dateCreated":"2019-05-18T13:10:33+0200","dateStarted":"2019-05-18T13:15:02+0200","dateFinished":"2019-05-18T13:15:03+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:333"},{"text":"//Cálculo de matrices de correlación según método especificado: Pearson (por defecto), Spearman.\n","user":"anonymous","dateUpdated":"2019-05-18T13:18:32+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558178082298_444683024","id":"20190518-131442_1180727943","dateCreated":"2019-05-18T13:14:42+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:334"},{"text":"import org.apache.spark.ml.linalg.{Matrix, Vectors}\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\nval data = Seq(\nVectors.sparse(4, Seq((0, 1.0), (3, -2.0))),\nVectors.dense(4.0, 5.0, 0.0, 3.0),\nVectors.dense(6.0, 7.0, 0.0, 8.0),\nVectors.sparse(4, Seq((0, 9.0), (3, 1.0))) )\nval df = data.map(Tuple1.apply).toDF(\"features\")\nval Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\nprintln(\"Pearson correlation matrix:\\n\" + coeff1.toString)\nval Row(coeff2: Matrix) = Correlation.corr(df, \"features\",\n\"spearman\").head\nprintln(\"Spearman correlation matrix:\\n\" + coeff2.toString)\n","user":"anonymous","dateUpdated":"2019-05-18T13:18:49+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Pearson correlation matrix:\n1.0                   0.055641488407465814  NaN  0.4004714203168137  \n0.055641488407465814  1.0                   NaN  0.9135958615342522  \nNaN                   NaN                   1.0  NaN                 \n0.4004714203168137    0.9135958615342522    NaN  1.0                 \nSpearman correlation matrix:\n1.0                  0.10540925533894532  NaN  0.40000000000000174  \n0.10540925533894532  1.0                  NaN  0.9486832980505141   \nNaN                  NaN                  1.0  NaN                  \n0.40000000000000174  0.9486832980505141   NaN  1.0                  \nimport org.apache.spark.ml.linalg.{Matrix, Vectors}\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\ndata: Seq[org.apache.spark.ml.linalg.Vector] = List((4,[0,3],[1.0,-2.0]), [4.0,5.0,0.0,3.0], [6.0,7.0,0.0,8.0], (4,[0,3],[9.0,1.0]))\ndf: org.apache.spark.sql.DataFrame = [features: vector]\ncoeff1: org.apache.spark.ml.linalg.Matrix =\n1.0                   0.055641488407465814  NaN  0.4004714203168137\n0.055641488407465814  1.0                   NaN  0.9135958615342522\nNaN                   NaN                   1.0  NaN\n0.4004714203168137    0.9135958615342522    NaN  1.0\ncoeff2: org.apache.spark.ml.linalg.Matrix =\n1.0                  0.10540925533894532  NaN  0.40000000000000174\n0.10540925533894532  1.0                  NaN  0.9486832980505141\nNaN              ..."}]},"apps":[],"jobName":"paragraph_1558178312259_-1460898572","id":"20190518-131832_1478759561","dateCreated":"2019-05-18T13:18:32+0200","dateStarted":"2019-05-18T13:18:49+0200","dateFinished":"2019-05-18T13:18:58+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:335"},{"text":"//Test de hipótesis para determinar si los resultados son estadísticamente significativos.\n//Utiliza test de χ \" de Pearson.\n","user":"anonymous","dateUpdated":"2019-05-18T13:20:12+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558178329060_606775946","id":"20190518-131849_369947222","dateCreated":"2019-05-18T13:18:49+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:336"},{"text":"import org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.stat.ChiSquareTest\nval data = Seq(\n(0.0, Vectors.dense(0.5,\n 10.0)),\n(0.0, Vectors.dense(1.5,\n 20.0)),\n(1.0, Vectors.dense(1.5,\n 30.0)),\n(0.0, Vectors.dense(3.5,\n 30.0)),\n(0.0, Vectors.dense(3.5,\n 40.0)),\n(1.0, Vectors.dense(3.5,\n 40.0)) )\nval df = data.toDF(\"label\", \"features\")\nval chi = ChiSquareTest.test(df, \"features\", \"label\").head\nprintln(\"pValues = \" + chi.getAs[Vector](0))\nprintln(\"degreesOfFreedom = \" +\nchi.getSeq[Int](1).mkString(\"[\", \",\", \"]\"))\nprintln(\"statistics = \" + chi.getAs[Vector](2))\n","user":"anonymous","dateUpdated":"2019-05-18T13:20:37+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pValues = [0.6872892787909721,0.6822703303362126]\ndegreesOfFreedom = [2,3]\nstatistics = [0.75,1.5]\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.stat.ChiSquareTest\ndata: Seq[(Double, org.apache.spark.ml.linalg.Vector)] = List((0.0,[0.5,10.0]), (0.0,[1.5,20.0]), (1.0,[1.5,30.0]), (0.0,[3.5,30.0]), (0.0,[3.5,40.0]), (1.0,[3.5,40.0]))\ndf: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nchi: org.apache.spark.sql.Row = [[0.6872892787909721,0.6822703303362126],WrappedArray(2, 3),[0.75,1.5]]\n"}]},"apps":[],"jobName":"paragraph_1558178412602_488027892","id":"20190518-132012_1985476167","dateCreated":"2019-05-18T13:20:12+0200","dateStarted":"2019-05-18T13:20:37+0200","dateFinished":"2019-05-18T13:20:39+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:337"},{"text":"//Muestreo de datos se puede hacer de maner estratificada, seleccionando por proporción de clave(clase).\n","user":"anonymous","dateUpdated":"2019-05-18T13:29:07+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558178437970_1626594086","id":"20190518-132037_361379985","dateCreated":"2019-05-18T13:20:37+0200","dateStarted":"2019-05-18T13:29:07+0200","dateFinished":"2019-05-18T13:29:07+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:338"},{"text":"// an RDD[(K, V)] of any key value pairs\nval data = sc.parallelize( Seq((1, 'a'), (1, 'b'), (2,\n'c'), (2, 'd'), (2, 'e'), (3, 'f')))\n// specify the exact fraction desired from each key\nval fractions = Map(1 -> 0.1, 2 -> 0.6, 3 -> 0.3)\n// Get an approximate sample from each stratum\nval approxSample = data.sampleByKey(withReplacement =\nfalse, fractions = fractions)\n// Get an exact sample from each stratum\nval exactSample = data.sampleByKeyExact(withReplacement =\nfalse, fractions = fractions)\n//Print the current proportion\nexactSample.map(_._1).countByValue\n","user":"anonymous","dateUpdated":"2019-05-18T13:31:59+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data: org.apache.spark.rdd.RDD[(Int, Char)] = ParallelCollectionRDD[34] at parallelize at <console>:37\nfractions: scala.collection.immutable.Map[Int,Double] = Map(1 -> 0.1, 2 -> 0.6, 3 -> 0.3)\napproxSample: org.apache.spark.rdd.RDD[(Int, Char)] = MapPartitionsRDD[35] at sampleByKey at <console>:42\nexactSample: org.apache.spark.rdd.RDD[(Int, Char)] = MapPartitionsRDD[37] at sampleByKeyExact at <console>:45\nres12: scala.collection.Map[Int,Long] = Map(1 -> 1, 3 -> 1, 2 -> 2)\n"}]},"apps":[],"jobName":"paragraph_1558178947711_1632491196","id":"20190518-132907_606843313","dateCreated":"2019-05-18T13:29:07+0200","dateStarted":"2019-05-18T13:31:59+0200","dateFinished":"2019-05-18T13:31:59+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:339"},{"text":"//Preprocesamiento de atributos Extracting, transforming and selecting features\n","user":"anonymous","dateUpdated":"2019-05-18T13:32:37+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558179119183_1871138226","id":"20190518-133159_1667854545","dateCreated":"2019-05-18T13:31:59+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:340"},{"text":"//Preprocesamiento de atributos Features Extractors\n","user":"anonymous","dateUpdated":"2019-05-18T13:39:12+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558179536698_-153517942","id":"20190518-133856_1729098373","dateCreated":"2019-05-18T13:38:56+0200","dateStarted":"2019-05-18T13:39:12+0200","dateFinished":"2019-05-18T13:39:12+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:341"},{"text":"import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\nval df = spark.createDataFrame(Seq(\n(0, Array(\"a\", \"b\", \"c\")),\n(1, Array(\"a\", \"b\", \"b\", \"c\", \"a\")) )).toDF(\"id\", \"words\")\n// fit a CountVectorizerModel from the corpus\nval cvModel: CountVectorizerModel = new CountVectorizer()\n.setInputCol(\"words\")\n.setOutputCol(\"features\")\n.setVocabSize(3)\n.setMinDF(2) .fit(df)\n// alternatively, define CountVectorizerModel with a-priori vocabulary\nval cvm = new CountVectorizerModel(Array(\"a\", \"b\", \"c\")).setInputCol(\"words\")\n.setOutputCol(\"features\")\ncvModel.transform(df).show(false)\n","user":"anonymous","dateUpdated":"2019-05-18T13:39:15+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\nimport org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\ndf: org.apache.spark.sql.DataFrame = [id: int, words: array<string>]\ncvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_57f815635cbb\ncvm: org.apache.spark.ml.feature.CountVectorizerModel = cntVecModel_93775f5264e5\n"}]},"apps":[],"jobName":"paragraph_1558179530698_1088689916","id":"20190518-133850_549917602","dateCreated":"2019-05-18T13:38:50+0200","dateStarted":"2019-05-18T13:39:15+0200","dateFinished":"2019-05-18T13:39:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:342"},{"text":"//StopWordsRemover\n","user":"anonymous","dateUpdated":"2019-05-18T13:40:27+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558179555955_1163997699","id":"20190518-133915_1712444477","dateCreated":"2019-05-18T13:39:15+0200","dateStarted":"2019-05-18T13:40:27+0200","dateFinished":"2019-05-18T13:40:28+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:343"},{"text":"import org.apache.spark.ml.feature.StopWordsRemover\nval remover = new StopWordsRemover().setInputCol(\"raw\")\n.setOutputCol(\"filtered\")\nval dataSet = spark.createDataFrame(Seq(\n(0, Seq(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n(1, Seq(\"Mary\", \"had\", \"a\", \"little\", \"lamb\")) )).toDF(\"id\", \"raw\")\nremover.transform(dataSet).show(false)\n","user":"anonymous","dateUpdated":"2019-05-18T13:40:38+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\nimport org.apache.spark.ml.feature.StopWordsRemover\nremover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_be57976ab920\ndataSet: org.apache.spark.sql.DataFrame = [id: int, raw: array<string>]\n"}]},"apps":[],"jobName":"paragraph_1558179627876_-2128785040","id":"20190518-134027_966295361","dateCreated":"2019-05-18T13:40:27+0200","dateStarted":"2019-05-18T13:40:38+0200","dateFinished":"2019-05-18T13:40:39+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:344"},{"text":"// •N-Gram\n// -\n//  Secuencia N-Gram: Secuencia de n tokens (normalmente palabras).\n// -\n//  Toma como entrada una secuencia de strings (por ejemplo, la salida\n// de Tokenizer). El parámetro n indica el número de términos en cada\n// n-gram.\n// -\n//  La salida será una secuencia de n-gramas, donde cada n-grama se\n// representa por una cadena de n palabras consecutivas.\n","user":"anonymous","dateUpdated":"2019-05-18T13:42:07+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558179638395_-1756077534","id":"20190518-134038_978772457","dateCreated":"2019-05-18T13:40:38+0200","dateStarted":"2019-05-18T13:42:07+0200","dateFinished":"2019-05-18T13:42:07+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:345"},{"text":"import org.apache.spark.ml.feature.NGram\nval wordDataFrame = spark.createDataFrame(Seq(\n(0, Array(\"Hi\", \"I\", \"heard\", \"about\", \"Spark\")),\n(1, Array(\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\")),\n(2, Array(\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"))\n)).toDF(\"id\", \"words\")\nval ngram = new NGram().setN(2).setInputCol(\"words\").setOutputCol(\"ngrams\")\nval ngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(false)\n","user":"anonymous","dateUpdated":"2019-05-18T13:42:22+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------------------------------------+\n|ngrams                                                            |\n+------------------------------------------------------------------+\n|[Hi I, I heard, heard about, about Spark]                         |\n|[I wish, wish Java, Java could, could use, use case, case classes]|\n|[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\nimport org.apache.spark.ml.feature.NGram\nwordDataFrame: org.apache.spark.sql.DataFrame = [id: int, words: array<string>]\nngram: org.apache.spark.ml.feature.NGram = ngram_f08597c23893\nngramDataFrame: org.apache.spark.sql.DataFrame = [id: int, words: array<string> ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1558179727607_-1002611146","id":"20190518-134207_448761231","dateCreated":"2019-05-18T13:42:07+0200","dateStarted":"2019-05-18T13:42:22+0200","dateFinished":"2019-05-18T13:42:23+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:346"},{"text":"// Binarizer\n// •\n//  Proceso de binarizar características numéricas a características binarias\n// (0/1)\n// •\n//  Recibe la columna de entrada (inputCol), columna de salida (outputCol) y\n// un umbral (threshold) para binarizar. Valores mayores que un umbral se\n// binarizan a 1, valores iguales o menores que un umbral se binarizan a 0.\n","user":"anonymous","dateUpdated":"2019-05-18T13:47:38+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558179742785_-1531426132","id":"20190518-134222_301028341","dateCreated":"2019-05-18T13:42:22+0200","dateStarted":"2019-05-18T13:47:38+0200","dateFinished":"2019-05-18T13:47:39+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:347"},{"text":"import org.apache.spark.ml.feature.Binarizer\nval data = Array((0, 0.1), (1, 0.8), (2, 0.2))\nval dataFrame = spark.createDataFrame(data).toDF(\"id\", \"feature\")\nval binarizer: Binarizer = new\nBinarizer().setInputCol(\"feature\").setOutputCol(\"binarized_feature\")\n.setThreshold(0.5)\nval binarizedDataFrame = binarizer.transform(dataFrame)\nprintln(s\"Binarizer output with Threshold = ${binarizer.getThreshold}\")\nbinarizedDataFrame.show()\n","user":"anonymous","dateUpdated":"2019-05-18T13:47:57+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Binarizer output with Threshold = 0.5\n+---+-------+-----------------+\n| id|feature|binarized_feature|\n+---+-------+-----------------+\n|  0|    0.1|              0.0|\n|  1|    0.8|              1.0|\n|  2|    0.2|              0.0|\n+---+-------+-----------------+\n\nimport org.apache.spark.ml.feature.Binarizer\ndata: Array[(Int, Double)] = Array((0,0.1), (1,0.8), (2,0.2))\ndataFrame: org.apache.spark.sql.DataFrame = [id: int, feature: double]\nbinarizer: org.apache.spark.ml.feature.Binarizer = binarizer_64e72e16f66a\nbinarizedDataFrame: org.apache.spark.sql.DataFrame = [id: int, feature: double ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1558180058915_582774822","id":"20190518-134738_1744152050","dateCreated":"2019-05-18T13:47:38+0200","dateStarted":"2019-05-18T13:47:57+0200","dateFinished":"2019-05-18T13:47:58+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:348"},{"text":"//PCA\n\n// Procedimiento estadístico que usa una transformación ortogonal para\n// convertir un conjunto de observaciones de variables posiblemente\n// correlacionadas en un conjunto de valores de variables no\n// correlacionadas linealmente (Componentes Principales).\n\n","user":"anonymous","dateUpdated":"2019-05-18T14:20:56+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558180077906_790000812","id":"20190518-134757_1939271518","dateCreated":"2019-05-18T13:47:57+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"text":"import org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.linalg.Vectors\nval data = Array(\nVectors.sparse(5, Seq((1, 1.0), (3, 7.0))),\nVectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\nVectors.dense(4.0, 0.0, 0.0, 6.0, 7.0) )\nval df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\nval pca = new\nPCA().setInputCol(\"features\").setOutputCol(\"pcaFeatures\").setK(3).fit(df)\nval result = pca.transform(df).select(\"pcaFeatures\")\nresult.show(false)\n","user":"anonymous","dateUpdated":"2019-05-18T13:56:39+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------------------------------------------------+\n|pcaFeatures                                                |\n+-----------------------------------------------------------+\n|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n+-----------------------------------------------------------+\n\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.linalg.Vectors\ndata: Array[org.apache.spark.ml.linalg.Vector] = Array((5,[1,3],[1.0,7.0]), [2.0,0.0,3.0,4.0,5.0], [4.0,0.0,0.0,6.0,7.0])\ndf: org.apache.spark.sql.DataFrame = [features: vector]\npca: org.apache.spark.ml.feature.PCAModel = pca_6cc71e995429\nresult: org.apache.spark.sql.DataFrame = [pcaFeatures: vector]\n"}]},"apps":[],"jobName":"paragraph_1558180198091_733775896","id":"20190518-134958_1702353014","dateCreated":"2019-05-18T13:49:58+0200","dateStarted":"2019-05-18T13:56:39+0200","dateFinished":"2019-05-18T13:56:39+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"// StringIndexer\n// •\n//  Codifica una columna de etiquetas categóricas (strings) a una columna de\n// etiquetas índices (double).\n// •\n//  Los índices se muestran en el intervalo [0, numLabels), ordenadas por\n// frecuencia, por tanto la etiqueta más frecuente obtiene el índice 0.\n","user":"anonymous","dateUpdated":"2019-05-18T14:23:04+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558182134706_-245754382","id":"20190518-142214_399483701","dateCreated":"2019-05-18T14:22:14+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"import org.apache.spark.ml.feature.StringIndexer\nval df = spark.createDataFrame(\nSeq((0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"))\n).toDF(\"id\", \"category\")\nval indexer = new StringIndexer() .setInputCol(\"category\")\n.setOutputCol(\"categoryIndex\")\nval indexed = indexer.fit(df).transform(df)\nprintln(s\"Transformed string column '${indexer.getInputCol}' \" + s\"to indexed column '${indexer.getOutputCol}'\")\nindexed.show()\n","user":"anonymous","dateUpdated":"2019-05-18T14:23:40+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Transformed string column 'category' to indexed column 'categoryIndex'\n+---+--------+-------------+\n| id|category|categoryIndex|\n+---+--------+-------------+\n|  0|       a|          0.0|\n|  1|       b|          2.0|\n|  2|       c|          1.0|\n|  3|       a|          0.0|\n|  4|       a|          0.0|\n|  5|       c|          1.0|\n+---+--------+-------------+\n\nimport org.apache.spark.ml.feature.StringIndexer\ndf: org.apache.spark.sql.DataFrame = [id: int, category: string]\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_10f21371c496\nindexed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1558180599058_1835743085","id":"20190518-135639_382011529","dateCreated":"2019-05-18T13:56:39+0200","dateStarted":"2019-05-18T14:23:40+0200","dateFinished":"2019-05-18T14:23:41+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"// IndexToString\n// •\n//  Deshace la transformación de StringIndexer: transforma una columna\n// de etiquetas índices a etiquetas de tipo String.\n// •\n//  Obtiene las etiquetas originales de un modelo entrenado con índices.\n","user":"anonymous","dateUpdated":"2019-05-18T14:27:27+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558182201660_-212439733","id":"20190518-142321_263465836","dateCreated":"2019-05-18T14:23:21+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"import org.apache.spark.ml.attribute.Attribute\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer}\nval df = spark.createDataFrame(Seq(\n  (0, \"a\"),\n  (1, \"b\"),\n  (2, \"c\"),\n  (3, \"a\"),\n  (4, \"a\"),\n  (5, \"c\")\n)).toDF(\"id\", \"category\")\n\nval indexer = new StringIndexer()\n  .setInputCol(\"category\")\n  .setOutputCol(\"categoryIndex\")\n  .fit(df)\nval indexed = indexer.transform(df)\n\nprintln(s\"Transformed string column '${indexer.getInputCol}' \" +\n    s\"to indexed column '${indexer.getOutputCol}'\")\nindexed.show()\n\nval inputColSchema = indexed.schema(indexer.getOutputCol)\nprintln(s\"StringIndexer will store labels in output column metadata: \" +\n    s\"${Attribute.fromStructField(inputColSchema).toString}\\n\")\n\nval converter = new IndexToString()\n  .setInputCol(\"categoryIndex\")\n  .setOutputCol(\"originalCategory\")\n\nval converted = converter.transform(indexed)\n\nprintln(s\"Transformed indexed column '${converter.getInputCol}' back to original string \" +\n    s\"column '${converter.getOutputCol}' using labels in metadata\")\nconverted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()","user":"anonymous","dateUpdated":"2019-05-18T14:31:37+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Transformed string column 'category' to indexed column 'categoryIndex'\n+---+--------+-------------+\n| id|category|categoryIndex|\n+---+--------+-------------+\n|  0|       a|          0.0|\n|  1|       b|          2.0|\n|  2|       c|          1.0|\n|  3|       a|          0.0|\n|  4|       a|          0.0|\n|  5|       c|          1.0|\n+---+--------+-------------+\n\nStringIndexer will store labels in output column metadata: {\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"categoryIndex\"}\n\nTransformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n+---+-------------+----------------+\n| id|categoryIndex|originalCategory|\n+---+-------------+----------------+\n|  0|          0.0|               a|\n|  1|          2.0|               b|\n|  2|          1.0|               c|\n|  3|          0.0|               a|\n|  4|          0.0|               a|\n|  5|          1.0|               c|\n+---+-------------+----------------+\n\nimport org.apache.spark.ml.attribute.Attribute\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer}\ndf: org.apache.spark.sql.DataFrame = [id: int, category: string]\nindexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_bd52d45c6fcb\nindexed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\ninputColSchema: org.apache.spark.sql.types.StructField = StructField(categoryIndex,DoubleType,true)\nconverter: org.apache.spark.ml.feature.IndexToString = idxToStr_266994a29d73\nconverted: org.apache.spark.sql.DataFrame = [id: int, category: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1558182447344_-1036700118","id":"20190518-142727_1410789651","dateCreated":"2019-05-18T14:27:27+0200","dateStarted":"2019-05-18T14:31:37+0200","dateFinished":"2019-05-18T14:31:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"text":"// OneHotEncoder\n// •\n//  Transforma una columna de etiquetas índices a una columna de vectores\n// binarios con un valor único como máximo.\n// •\n//  Permite el uso de algoritmos (como Logistic Regression) que esperan\n// funciones continuas, sobre datos categóricos.\n","user":"anonymous","dateUpdated":"2019-05-18T14:38:05+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558182470278_1842589674","id":"20190518-142750_538114464","dateCreated":"2019-05-18T14:27:50+0200","dateStarted":"2019-05-18T14:38:05+0200","dateFinished":"2019-05-18T14:38:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:355"},{"text":"import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nval df = spark.createDataFrame(Seq( (0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"),\n(4, \"a\"), (5, \"c\") )).toDF(\"id\", \"category\")\nval indexer = new StringIndexer()\n.setInputCol(\"category\").setOutputCol(\"categoryIndex\") .fit(df)\nval indexed = indexer.transform(df)\nval encoder = new\nOneHotEncoder().setInputCol(\"categoryIndex\").setOutputCol(\"categoryVec\")\nval encoded = encoder.transform(indexed)\nencoded.show()\n","user":"anonymous","dateUpdated":"2019-05-18T14:41:05+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+-------------+-------------+\n| id|category|categoryIndex|  categoryVec|\n+---+--------+-------------+-------------+\n|  0|       a|          0.0|(2,[0],[1.0])|\n|  1|       b|          2.0|    (2,[],[])|\n|  2|       c|          1.0|(2,[1],[1.0])|\n|  3|       a|          0.0|(2,[0],[1.0])|\n|  4|       a|          0.0|(2,[0],[1.0])|\n|  5|       c|          1.0|(2,[1],[1.0])|\n+---+--------+-------------+-------------+\n\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\ndf: org.apache.spark.sql.DataFrame = [id: int, category: string]\nindexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_dd7f4d0638cc\nindexed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\nencoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_20bcf7c34122\nencoded: org.apache.spark.sql.DataFrame = [id: int, category: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1558183085928_103339628","id":"20190518-143805_607759672","dateCreated":"2019-05-18T14:38:05+0200","dateStarted":"2019-05-18T14:41:05+0200","dateFinished":"2019-05-18T14:41:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558183265260_10836105","id":"20190518-144105_1442625660","dateCreated":"2019-05-18T14:41:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:357","text":"// VectorAssembler\n// •\n//  “Transformer” que combina una lista de columnas dadas en una única\n// columna que contiene un vector.\n// •\n//  Útil para combinar características originales con otras generadas a partir\n// de transformaciones en un solo vector de características, para entrenar\n// modelos ML como logistic regression o decision trees.\n","dateUpdated":"2019-05-18T16:47:45+0200","dateFinished":"2019-05-18T16:48:02+0200","dateStarted":"2019-05-18T16:47:46+0200","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558190865716_891951131","id":"20190518-164745_25583589","dateCreated":"2019-05-18T16:47:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2629","text":"import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nval dataset = spark.createDataFrame(\nSeq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))\n).toDF(\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\")\ndataset.show()\nval assembler = new VectorAssembler().setInputCols(Array(\"hour\", \"mobile\",\n\"userFeatures\")) .setOutputCol(\"features\")\nval output = assembler.transform(dataset)\nprintln(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column'features'\")\noutput.select(\"features\", \"clicked\").show(false)\n","dateUpdated":"2019-05-18T16:49:47+0200","dateFinished":"2019-05-18T16:49:47+0200","dateStarted":"2019-05-18T16:49:47+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----+------+--------------+-------+\n| id|hour|mobile|  userFeatures|clicked|\n+---+----+------+--------------+-------+\n|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n+---+----+------+--------------+-------+\n\nAssembled columns 'hour', 'mobile', 'userFeatures' to vector column'features'\n+-----------------------+-------+\n|features               |clicked|\n+-----------------------+-------+\n|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n+-----------------------+-------+\n\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\ndataset: org.apache.spark.sql.DataFrame = [id: int, hour: int ... 3 more fields]\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_571cfce5fb67\noutput: org.apache.spark.sql.DataFrame = [id: int, hour: int ... 4 more fields]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558190953119_1322670282","id":"20190518-164913_1567628303","dateCreated":"2019-05-18T16:49:13+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2727","text":"// Interation\n// •\n//  Es un “Transformer” que genera una columna con un vector que contiene\n// el producto de todas las combinaciones de un valor de cada columna de\n// entrada.\n// •\n//  Con dos columnas conteniendo vectores de dimensión 3, generará una\n// columna de salida que contiene un vector de dimensión 9.\n","dateUpdated":"2019-05-18T16:50:11+0200","dateFinished":"2019-05-18T16:50:11+0200","dateStarted":"2019-05-18T16:50:11+0200","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558191011199_1191359226","id":"20190518-165011_199918556","dateCreated":"2019-05-18T16:50:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2872","text":"import org.apache.spark.ml.feature.Interaction\nimport org.apache.spark.ml.feature.VectorAssembler\nval df = spark.createDataFrame(Seq(\n    \n(1, 1, 2, 3, 8, 4, 5),\n(2, 4, 3, 8, 7, 9, 8),\n(3, 6, 1, 9, 2, 3, 6),\n(4, 10, 8, 6, 9, 4, 5),\n(5, 9, 2, 7, 10, 7, 3),\n(6, 1, 1, 4, 2, 8, 4) ))\n.toDF(\"id1\", \"id2\", \"id3\", \"id4\", \"id5\", \"id6\", \"id7\")\n\ndf.show()\nval assembler1 = new VectorAssembler(). setInputCols(Array(\"id2\", \"id3\",\n\"id4\")). setOutputCol(\"vec1\")\nval assembled1 = assembler1.transform(df)\nval assembler2 = new VectorAssembler(). setInputCols(Array(\"id5\", \"id6\",\n\"id7\")). setOutputCol(\"vec2\")\nval assembled2 = assembler2.transform(assembled1).select(\"id1\", \"vec1\", \"vec2\")\nval interaction = new Interaction() .setInputCols(Array(\"id1\", \"vec1\", \"vec2\"))\n.setOutputCol(\"interactedCol\")\nval interacted = interaction.transform(assembled2) \ninteracted.show(truncate =false)\n","dateUpdated":"2019-05-18T16:58:47+0200","dateFinished":"2019-05-18T16:58:47+0200","dateStarted":"2019-05-18T16:58:47+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---+---+---+---+---+---+\n|id1|id2|id3|id4|id5|id6|id7|\n+---+---+---+---+---+---+---+\n|  1|  1|  2|  3|  8|  4|  5|\n|  2|  4|  3|  8|  7|  9|  8|\n|  3|  6|  1|  9|  2|  3|  6|\n|  4| 10|  8|  6|  9|  4|  5|\n|  5|  9|  2|  7| 10|  7|  3|\n|  6|  1|  1|  4|  2|  8|  4|\n+---+---+---+---+---+---+---+\n\n+---+--------------+--------------+------------------------------------------------------+\n|id1|vec1          |vec2          |interactedCol                                         |\n+---+--------------+--------------+------------------------------------------------------+\n|1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]            |\n|2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] |[56.0,72.0,64.0,42.0,54.0,48.0,112.0,144.0,128.0]     |\n|3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] |[36.0,54.0,108.0,6.0,9.0,18.0,54.0,81.0,162.0]        |\n|4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] |[360.0,160.0,200.0,288.0,128.0,160.0,216.0,96.0,120.0]|\n|5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]|[450.0,315.0,135.0,100.0,70.0,30.0,350.0,245.0,105.0] |\n|6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] |[12.0,48.0,24.0,12.0,48.0,24.0,48.0,192.0,96.0]       |\n+---+--------------+--------------+------------------------------------------------------+\n\nimport org.apache.spark.ml.feature.Interaction\nimport org.apache.spark.ml.feature.VectorAssembler\ndf: org.apache.spark.sql.DataFrame = [id1: int, id2: int ... 5 more fields]\nassembler1: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e124befe17b9\nassembled1: org.apache.spark.sql.DataFrame = [id1: int, id2: int ... 6 more fields]\nassembler2: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_968054d012b9\nassembled2: org.apache.spark.sql.DataFrame = [id1: int, vec1: vector ... 1 more field]\ninteraction: org.apache.spark.ml.feature.Interaction = interaction_0c3181d8960d\ninteracted: org.apache.spark.sql.DataFrame = [id1: int, vec1: vector ... 2 more fields]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558191465155_411860848","id":"20190518-165745_341933860","dateCreated":"2019-05-18T16:57:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2953","text":"//Normalizer\n// •\n//  “Transformer” que estandariza los datos de entrada y puede mejorar el\n// comportamiento de algoritmos de aprendizaje.\n","dateUpdated":"2019-05-18T17:00:02+0200","dateFinished":"2019-05-18T17:00:03+0200","dateStarted":"2019-05-18T17:00:03+0200","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558191602951_-387150513","id":"20190518-170002_928968293","dateCreated":"2019-05-18T17:00:02+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3106","text":"import  org.apache.spark.ml.feature.Normalizer\n// Tiene un parámetro para especificar la p-norma usada en la  normalización (por defecto p = 2).\n\nimport org.apache.spark.ml.linalg.Vectors\nval dataFrame\n = spark.createDataFrame(Seq(\n(0,\n Vectors.dense(1.0, 0.5, -1.0)),\n(1,\n Vectors.dense(2.0, 1.0, 1.0)),\n(2,\n Vectors.dense(4.0, 10.0, 2.0)) )).toDF(\"id\", \"features\")\n \n dataFrame.show()\n// Normalize each Vector using $L^1$ norm.\nval normalizer = new Normalizer().setInputCol(\"features\")\n.setOutputCol(\"normFeatures\").setP(1.0)\nval l1NormData = normalizer.transform(dataFrame) \nprintln(\"Normalized using L^1 norm\")\nl1NormData.show()\n// Normalize each Vector using $L^\\infty$ norm.\nval lInfNormData = normalizer.transform(dataFrame,\nnormalizer.p -> Double.PositiveInfinity)\nprintln(\"Normalized using L^inf norm\")\nlInfNormData.show()\n","dateUpdated":"2019-05-18T17:29:10+0200","dateFinished":"2019-05-18T17:29:10+0200","dateStarted":"2019-05-18T17:29:10+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------------+\n| id|      features|\n+---+--------------+\n|  0|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]|\n|  2|[4.0,10.0,2.0]|\n+---+--------------+\n\nNormalized using L^1 norm\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\nNormalized using L^inf norm\n+---+--------------+--------------+\n| id|      features|  normFeatures|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\nimport org.apache.spark.ml.feature.Normalizer\nimport org.apache.spark.ml.linalg.Vectors\ndataFrame: org.apache.spark.sql.DataFrame = [id: int, features: vector]\nnormalizer: org.apache.spark.ml.feature.Normalizer = normalizer_715c646a5f19\nl1NormData: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 1 more field]\nlInfNormData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, features: vector ... 1 more field]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558191655528_1962498384","id":"20190518-170055_1822323182","dateCreated":"2019-05-18T17:00:55+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3187","text":"//MinMaxScaler\n// •\n// Reescala las características a un rango específico (normalmente [0, 1]).\n// Parámetros:\n// -\n//  min: 0.0 por defecto. Límite inferior después de la transformación, compartido\n// por todas las características.\n// -\n//  max: 1.0 por defecto. Límite superior después de la transformación, compartido\n// por todas las características.\n","dateUpdated":"2019-05-18T17:29:59+0200","dateFinished":"2019-05-18T17:29:59+0200","dateStarted":"2019-05-18T17:29:59+0200","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558193399576_-1422618720","id":"20190518-172959_1899826004","dateCreated":"2019-05-18T17:29:59+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3332","text":"import org.apache.spark.ml.feature.MinMaxScaler\nimport org.apache.spark.ml.linalg.Vectors\nval dataFrame = spark.createDataFrame(Seq(\n(0, Vectors.dense(1.0, 0.1, -1.0)), (1, Vectors.dense(2.0, 1.1,\n1.0)), (2, Vectors.dense(3.0, 10.1, 3.0)) )).toDF(\"id\", \"features\")\nval scaler = new MinMaxScaler()\n.setInputCol(\"features\").setOutputCol(\"scaledFeatures\")\n// Compute summary statistics and generate MinMaxScalerModel\nval scalerModel = scaler.fit(dataFrame)\n// rescale each feature to range [min, max].\nval scaledData = scalerModel.transform(dataFrame)\nprintln(s\"Features scaled to range: [${scaler.getMin}, ${scaler.getMax}]\")\n\nscaledData.select(\"features\",\n \"scaledFeatures\").show()\n","dateUpdated":"2019-05-18T17:31:06+0200","dateFinished":"2019-05-18T17:31:10+0200","dateStarted":"2019-05-18T17:31:06+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Features scaled to range: [0.0, 1.0]\n+--------------+--------------+\n|      features|scaledFeatures|\n+--------------+--------------+\n|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|\n| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n+--------------+--------------+\n\nimport org.apache.spark.ml.feature.MinMaxScaler\nimport org.apache.spark.ml.linalg.Vectors\ndataFrame: org.apache.spark.sql.DataFrame = [id: int, features: vector]\nscaler: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_a216440c6e70\nscalerModel: org.apache.spark.ml.feature.MinMaxScalerModel = minMaxScal_a216440c6e70\nscaledData: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.100.198.6:4040/jobs/job?id=0"],"interpreterSettingId":"spark"}}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558193432301_-299587524","id":"20190518-173032_44711374","dateCreated":"2019-05-18T17:30:32+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3413","text":"//Feature Selectors: Seleccionar un subconjunto a partir de un gran conjunto de características\n","dateUpdated":"2019-05-18T17:31:35+0200","dateFinished":"2019-05-18T17:31:35+0200","dateStarted":"2019-05-18T17:31:35+0200","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558193495097_249739043","id":"20190518-173135_52598580","dateCreated":"2019-05-18T17:31:35+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3574","text":"// VectorSlicer\n// •\n//  “Transformer” que toma un vector de características y genera un nuevo\n// vector de características con un sub-array de las características\n// originales.\n// •\n//  VectorSlicer recibe una columna vector y devuelve una nueva columna\n// vector que contiene los valores seleccionados a partir de los índices\n// especificados.\n// •\n//  Los índices pueden ser enteros representando los índices o String que\n// representan los nombres de las características en el vector.\n","dateUpdated":"2019-05-18T17:31:50+0200","dateFinished":"2019-05-18T17:31:50+0200","dateStarted":"2019-05-18T17:31:50+0200","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558193510521_1764793546","id":"20190518-173150_129521893","dateCreated":"2019-05-18T17:31:50+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3655","text":"import\n java.util.Arrays\nimport\n org.apache.spark.ml.attribute.{Attribute, AttributeGroup, NumericAttribute}\nimport\n org.apache.spark.ml.feature.VectorSlicer\nimport\n org.apache.spark.ml.linalg.Vectors\nimport\n org.apache.spark.sql.Row\nimport\n org.apache.spark.sql.types.StructType\n \n \n \nval data = Arrays.asList(\nRow(Vectors.sparse(3, Seq((0, -2.0), (1, 2.3)))),\nRow(Vectors.dense(-2.0, 2.3, 0.0)) )\nval defaultAttr = NumericAttribute.defaultAttr\nval attrs = Array(\"f1\", \"f2\", \"f3\").map(defaultAttr.withName)\nval attrGroup = new AttributeGroup(\"userFeatures\",attrs.asInstanceOf[Array[Attribute]])\nval dataset = spark.createDataFrame(data, StructType(Array(attrGroup.toStructField())))\nval slicer = new VectorSlicer().setInputCol(\"userFeatures\").setOutputCol(\"features\")\nslicer.setIndices(Array(1)).setNames(Array(\"f3\"))\n// or slicer.setIndices(Array(1, 2)), or slicer.setNames(Array(\"f2\", \"f3\"))\nval output = slicer.transform(dataset) \noutput.show(false)\n","dateUpdated":"2019-05-18T17:32:52+0200","dateFinished":"2019-05-18T17:32:53+0200","dateStarted":"2019-05-18T17:32:52+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-------------+\n|userFeatures        |features     |\n+--------------------+-------------+\n|(3,[0,1],[-2.0,2.3])|(2,[0],[2.3])|\n|[-2.0,2.3,0.0]      |[2.3,0.0]    |\n+--------------------+-------------+\n\nimport java.util.Arrays\nimport org.apache.spark.ml.attribute.{Attribute, AttributeGroup, NumericAttribute}\nimport org.apache.spark.ml.feature.VectorSlicer\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.StructType\ndata: java.util.List[org.apache.spark.sql.Row] = [[(3,[0,1],[-2.0,2.3])], [[-2.0,2.3,0.0]]]\ndefaultAttr: org.apache.spark.ml.attribute.NumericAttribute = {\"type\":\"numeric\"}\nattrs: Array[org.apache.spark.ml.attribute.NumericAttribute] = Array({\"type\":\"numeric\",\"name\":\"f1\"}, {\"type\":\"numeric\",\"name\":\"f2\"}, {\"type\":\"numeric\",\"name\":\"f3\"})\nattrGroup: org.apache.spark.ml.attribute.AttributeGroup = {\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"f1\"},{\"idx\":1,\"name\":\"f2\"},{\"idx\":2,\"name\":\"f3\"}]},\"num_attrs\":3}}\ndataset: or..."}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558193557755_2022404498","id":"20190518-173237_498042220","dateCreated":"2019-05-18T17:32:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3736","text":"// •ChiSqSelector\n\n//  Utiliza test de independencia χ \" para seleccionar las características.\n// •\n// Utiliza cinco métodos de selección:\n// -\n//  numTopFeatures: Selecciona un número fijo de características, aquellas con\n// mayor poder predictivo.\n// -\n//  percentile: Similar a numTopFeatures pero selecciona un porcentaje en lugar\n// de un número fijo.\n// -\n//  fpr: Selecciona todas las características cuyos p-values están por debajo de un\n// umbral\n// -\n//  fdr: Usa el procedimiento de Benjamini-Hochberg para elegir todas las\n// características cuya tasa de falsos positivos están por debajo de un umbral.\n// -\n//  fwe: Selecciona todas las características cuyos p-values están por debajo de un\n// umbral.\n","dateUpdated":"2019-05-18T17:38:17+0200","dateFinished":"2019-05-18T17:38:17+0200","dateStarted":"2019-05-18T17:38:17+0200","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558193897512_1542359582","id":"20190518-173817_1292276532","dateCreated":"2019-05-18T17:38:17+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3849","text":"import org.apache.spark.ml.feature.ChiSqSelector\nimport org.apache.spark.ml.linalg.Vectors\nval data = Seq(\n(7, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0),\n(8, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0),\n(9, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0) )\nval df = spark.createDataset(data).toDF(\"id\", \"features\", \"clicked\")\nval selector = new ChiSqSelector()\n.setNumTopFeatures(1).setFeaturesCol(\"features\")\n.setLabelCol(\"clicked\").setOutputCol(\"selectedFeatures\")\nval result = selector.fit(df).transform(df)\nprintln(s\"ChiSqSelector output with top ${selector.getNumTopFeatures}features selected\")\nresult.show()\n","dateUpdated":"2019-05-18T17:38:36+0200","dateFinished":"2019-05-18T17:38:38+0200","dateStarted":"2019-05-18T17:38:36+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ChiSqSelector output with top 1features selected\n+---+------------------+-------+----------------+\n| id|          features|clicked|selectedFeatures|\n+---+------------------+-------+----------------+\n|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n+---+------------------+-------+----------------+\n\nimport org.apache.spark.ml.feature.ChiSqSelector\nimport org.apache.spark.ml.linalg.Vectors\ndata: Seq[(Int, org.apache.spark.ml.linalg.Vector, Double)] = List((7,[0.0,0.0,18.0,1.0],1.0), (8,[0.0,1.0,12.0,0.0],0.0), (9,[1.0,0.0,15.0,0.1],0.0))\ndf: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 1 more field]\nselector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_a51c6bb2d5a0\nresult: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 2 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.100.198.6:4040/jobs/job?id=1","http://10.100.198.6:4040/jobs/job?id=2"],"interpreterSettingId":"spark"}}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558193916066_-1733928926","id":"20190518-173836_1960974620","dateCreated":"2019-05-18T17:38:36+0200","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3930"}],"name":"MLibClase","id":"2ECQEU6EC","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}