{"paragraphs":[{"text":"//////ACTIVIDADES MLlib //////","user":"anonymous","dateUpdated":"2019-05-18T18:10:17+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558195562852_1026491014","id":"20190518-180602_1892818614","dateCreated":"2019-05-18T18:06:02+0200","dateStarted":"2019-05-18T18:10:17+0200","dateFinished":"2019-05-18T18:10:17+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:426"},{"text":"//EJERCICIO 1 Estadísticas y preprocesamiento","user":"anonymous","dateUpdated":"2019-05-18T18:34:43+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558197269732_-1165333465","id":"20190518-183429_1856029379","dateCreated":"2019-05-18T18:34:29+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:427"},{"text":"//Estadísticas simples de los datos: la media, la varianza el num de ceros por columna.\n","user":"anonymous","dateUpdated":"2019-05-18T18:35:07+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558197269469_-457098319","id":"20190518-183429_1708028783","dateCreated":"2019-05-18T18:34:29+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:428"},{"text":"import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n\nval observations = sc.parallelize(\n  Seq(\n    Vectors.dense(1.0, 10.0, 100.0),\n    Vectors.dense(2.0, 20.0, 200.0),\n    Vectors.dense(3.0, 30.0, 300.0)\n  )\n)\n\n// Compute column summary statistics.\nval summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\nprintln(summary.mean)  // a dense vector containing the mean value for each column\nprintln(summary.variance)  // column-wise variance\nprintln(summary.numNonzeros)  // number of nonzeros in each column","user":"anonymous","dateUpdated":"2019-05-18T18:35:32+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[2.0,20.0,200.0]\n[1.0,100.0,10000.0]\n[3.0,3.0,3.0]\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nobservations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[43] at parallelize at <console>:63\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@5041bb95\n"}]},"apps":[],"jobName":"paragraph_1558197307284_1284121480","id":"20190518-183507_1898765026","dateCreated":"2019-05-18T18:35:07+0200","dateStarted":"2019-05-18T18:35:32+0200","dateFinished":"2019-05-18T18:35:32+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:429"},{"text":"//Estadísticas simples de los datos: número de elementos,mínimo, máximo.\n","user":"anonymous","dateUpdated":"2019-05-18T18:37:03+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558197269274_-1630676159","id":"20190518-183429_1528182818","dateCreated":"2019-05-18T18:34:29+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:430"},{"text":"import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}\nval observations = sc.parallelize(Seq(\nVectors.dense(1.0, 10.0, 100.0),\nVectors.dense(2.0, 20.0, 200.0),\nVectors.dense(3.0, 30.0, 300.0)))\n// Compute column summary statistics.\nval summary: MultivariateStatisticalSummary =Statistics.colStats(observations)\n// sample size\nprintln(summary.count)\n// Maximum value of each column\nprintln(summary.max)\n// Minimum value of each column\nprintln(summary.min)\n","user":"anonymous","dateUpdated":"2019-05-18T18:37:28+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"3\n[3.0,30.0,300.0]\n[1.0,10.0,100.0]\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nobservations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[45] at parallelize at <console>:67\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@5099b55f\n"}]},"apps":[],"jobName":"paragraph_1558197269074_1503315679","id":"20190518-183429_2030623362","dateCreated":"2019-05-18T18:34:29+0200","dateStarted":"2019-05-18T18:37:28+0200","dateFinished":"2019-05-18T18:37:28+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:431"},{"text":"////Cálculo de matrices de correlación según método especificado: Pearson (por defecto), Spearman.\n","user":"anonymous","dateUpdated":"2019-05-18T18:38:12+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558197268836_841712355","id":"20190518-183428_1020432548","dateCreated":"2019-05-18T18:34:28+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:432"},{"text":"import org.apache.spark.ml.linalg.{Matrix, Vectors}\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\nval data = Seq(\nVectors.sparse(4, Seq((0, 1.0), (3, -2.0))),\nVectors.dense(4.0, 5.0, 0.0, 3.0),\nVectors.dense(6.0, 7.0, 0.0, 8.0),\nVectors.sparse(4, Seq((0, 9.0), (3, 1.0))) )\nval df = data.map(Tuple1.apply).toDF(\"features\")\nval Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\nprintln(\"Pearson correlation matrix:\\n\" + coeff1.toString)\nval Row(coeff2: Matrix) = Correlation.corr(df, \"features\",\n\"spearman\").head\nprintln(\"Spearman correlation matrix:\\n\" + coeff2.toString)\n","user":"anonymous","dateUpdated":"2019-05-18T18:38:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Pearson correlation matrix:\n1.0                   0.055641488407465814  NaN  0.4004714203168137  \n0.055641488407465814  1.0                   NaN  0.9135958615342522  \nNaN                   NaN                   1.0  NaN                 \n0.4004714203168137    0.9135958615342522    NaN  1.0                 \nSpearman correlation matrix:\n1.0                  0.10540925533894532  NaN  0.40000000000000174  \n0.10540925533894532  1.0                  NaN  0.9486832980505141   \nNaN                  NaN                  1.0  NaN                  \n0.40000000000000174  0.9486832980505141   NaN  1.0                  \nimport org.apache.spark.ml.linalg.{Matrix, Vectors}\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\ndata: Seq[org.apache.spark.ml.linalg.Vector] = List((4,[0,3],[1.0,-2.0]), [4.0,5.0,0.0,3.0], [6.0,7.0,0.0,8.0], (4,[0,3],[9.0,1.0]))\ndf: org.apache.spark.sql.DataFrame = [features: vector]\ncoeff1: org.apache.spark.ml.linalg.Matrix =\n1.0                   0.055641488407465814  NaN  0.4004714203168137\n0.055641488407465814  1.0                   NaN  0.9135958615342522\nNaN                   NaN                   1.0  NaN\n0.4004714203168137    0.9135958615342522    NaN  1.0\ncoeff2: org.apache.spark.ml.linalg.Matrix =\n1.0                  0.10540925533894532  NaN  0.40000000000000174\n0.10540925533894532  1.0                  NaN  0.9486832980505141\nNaN              ..."}]},"apps":[],"jobName":"paragraph_1558197508578_-459392791","id":"20190518-183828_1745380458","dateCreated":"2019-05-18T18:38:28+0200","dateStarted":"2019-05-18T18:38:31+0200","dateFinished":"2019-05-18T18:38:32+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:433"},{"text":"//Test de hipótesis para determinar si los resultados son estadísticamente significativos.\n//Utiliza test de χ \" de Pearson.","user":"anonymous","dateUpdated":"2019-05-18T18:39:29+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558197268587_-793251619","id":"20190518-183428_1150637666","dateCreated":"2019-05-18T18:34:28+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:434"},{"text":"import org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.stat.ChiSquareTest\nval data = Seq(\n(0.0, Vectors.dense(0.5,\n 10.0)),\n(0.0, Vectors.dense(1.5,\n 20.0)),\n(1.0, Vectors.dense(1.5,\n 30.0)),\n(0.0, Vectors.dense(3.5,\n 30.0)),\n(0.0, Vectors.dense(3.5,\n 40.0)),\n(1.0, Vectors.dense(3.5,\n 40.0)) )\nval df = data.toDF(\"label\", \"features\")\nval chi = ChiSquareTest.test(df, \"features\", \"label\").head\nprintln(\"pValues = \" + chi.getAs[Vector](0))\nprintln(\"degreesOfFreedom = \" +\nchi.getSeq[Int](1).mkString(\"[\", \",\", \"]\"))\nprintln(\"statistics = \" + chi.getAs[Vector](2))\n","user":"anonymous","dateUpdated":"2019-05-18T18:39:37+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pValues = [0.6872892787909721,0.6822703303362126]\ndegreesOfFreedom = [2,3]\nstatistics = [0.75,1.5]\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.stat.ChiSquareTest\ndata: Seq[(Double, org.apache.spark.ml.linalg.Vector)] = List((0.0,[0.5,10.0]), (0.0,[1.5,20.0]), (1.0,[1.5,30.0]), (0.0,[3.5,30.0]), (0.0,[3.5,40.0]), (1.0,[3.5,40.0]))\ndf: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nchi: org.apache.spark.sql.Row = [[0.6872892787909721,0.6822703303362126],WrappedArray(2, 3),[0.75,1.5]]\n"}]},"apps":[],"jobName":"paragraph_1558197268324_-548626083","id":"20190518-183428_1337371617","dateCreated":"2019-05-18T18:34:28+0200","dateStarted":"2019-05-18T18:39:37+0200","dateFinished":"2019-05-18T18:39:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:435"},{"text":"//EJERCICIO 2           Modelos de conocimiento","user":"anonymous","dateUpdated":"2019-05-18T18:34:24+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558195562270_1395703769","id":"20190518-180602_81877766","dateCreated":"2019-05-18T18:06:02+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:436"},{"text":"//Stratified sampling\n","user":"anonymous","dateUpdated":"2019-05-18T18:10:00+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558195566036_-1043011641","id":"20190518-180606_390122127","dateCreated":"2019-05-18T18:06:06+0200","dateStarted":"2019-05-18T18:10:00+0200","dateFinished":"2019-05-18T18:10:00+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:437"},{"text":"\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject StratifiedSamplingExample {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf().setAppName(\"StratifiedSamplingExample\")\n    val sc = new SparkContext(conf)\n\n    // $example on$\n    // an RDD[(K, V)] of any key value pairs\n    val data = sc.parallelize(\n      Seq((1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')))\n\n    // specify the exact fraction desired from each key\n    val fractions = Map(1 -> 0.1, 2 -> 0.6, 3 -> 0.3)\n\n    // Get an approximate sample from each stratum\n    val approxSample = data.sampleByKey(withReplacement = false, fractions = fractions)\n    // Get an exact sample from each stratum\n    val exactSample = data.sampleByKeyExact(withReplacement = false, fractions = fractions)\n    // $example off$\n    \n    println(s\"approxSample size is ${approxSample.collect().size}\")\n    approxSample.collect().foreach(println)\n\n    println(s\"exactSample its size is ${exactSample.collect().size}\")\n    exactSample.collect().foreach(println)\n    sc.stop()\n  }\n}","user":"anonymous","dateUpdated":"2019-05-18T18:13:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.{SparkConf, SparkContext}\ndefined object StratifiedSamplingExample\n"}]},"apps":[],"jobName":"paragraph_1558194108109_-1179506799","id":"20190518-174148_1306843817","dateCreated":"2019-05-18T17:41:48+0200","dateStarted":"2019-05-18T18:13:07+0200","dateFinished":"2019-05-18T18:13:07+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:438"},{"text":"//CountVectorizer","user":"anonymous","dateUpdated":"2019-05-18T18:18:39+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558196289324_1413158132","id":"20190518-181809_548546827","dateCreated":"2019-05-18T18:18:09+0200","dateStarted":"2019-05-18T18:18:39+0200","dateFinished":"2019-05-18T18:18:39+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:439"},{"text":"import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n\nval df = spark.createDataFrame(Seq(\n  (0, Array(\"a\", \"b\", \"c\")),\n  (1, Array(\"a\", \"b\", \"b\", \"c\", \"a\"))\n)).toDF(\"id\", \"words\")\n\n// fit a CountVectorizerModel from the corpus\nval cvModel: CountVectorizerModel = new CountVectorizer()\n  .setInputCol(\"words\")\n  .setOutputCol(\"features\")\n  .setVocabSize(3)\n  .setMinDF(2)\n  .fit(df)\n\n// alternatively, define CountVectorizerModel with a-priori vocabulary\nval cvm = new CountVectorizerModel(Array(\"a\", \"b\", \"c\"))\n  .setInputCol(\"words\")\n  .setOutputCol(\"features\")\n\ncvModel.transform(df).show(false)","user":"anonymous","dateUpdated":"2019-05-18T18:18:13+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\nimport org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\ndf: org.apache.spark.sql.DataFrame = [id: int, words: array<string>]\ncvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_0d37513b58fe\ncvm: org.apache.spark.ml.feature.CountVectorizerModel = cntVecModel_a3d58eddaa61\n"}]},"apps":[],"jobName":"paragraph_1558196287533_-1086768627","id":"20190518-181807_1727043915","dateCreated":"2019-05-18T18:18:07+0200","dateStarted":"2019-05-18T18:18:13+0200","dateFinished":"2019-05-18T18:18:14+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:440"},{"text":"//Binarizer","user":"anonymous","dateUpdated":"2019-05-18T18:20:38+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558196293924_683777271","id":"20190518-181813_254205498","dateCreated":"2019-05-18T18:18:13+0200","dateStarted":"2019-05-18T18:20:38+0200","dateFinished":"2019-05-18T18:20:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:441"},{"text":"import org.apache.spark.ml.feature.Binarizer\n\nval data = Array((0, 0.1), (1, 0.8), (2, 0.2))\nval dataFrame = spark.createDataFrame(data).toDF(\"id\", \"feature\")\n\nval binarizer: Binarizer = new Binarizer()\n  .setInputCol(\"feature\")\n  .setOutputCol(\"binarized_feature\")\n  .setThreshold(0.5)\n\nval binarizedDataFrame = binarizer.transform(dataFrame)\n\nprintln(s\"Binarizer output with Threshold = ${binarizer.getThreshold}\")\nbinarizedDataFrame.show()","user":"anonymous","dateUpdated":"2019-05-18T18:20:41+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Binarizer output with Threshold = 0.5\n+---+-------+-----------------+\n| id|feature|binarized_feature|\n+---+-------+-----------------+\n|  0|    0.1|              0.0|\n|  1|    0.8|              1.0|\n|  2|    0.2|              0.0|\n+---+-------+-----------------+\n\nimport org.apache.spark.ml.feature.Binarizer\ndata: Array[(Int, Double)] = Array((0,0.1), (1,0.8), (2,0.2))\ndataFrame: org.apache.spark.sql.DataFrame = [id: int, feature: double]\nbinarizer: org.apache.spark.ml.feature.Binarizer = binarizer_f7fc2ab8efbf\nbinarizedDataFrame: org.apache.spark.sql.DataFrame = [id: int, feature: double ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1558196430740_-798657936","id":"20190518-182030_1145914925","dateCreated":"2019-05-18T18:20:30+0200","dateStarted":"2019-05-18T18:20:41+0200","dateFinished":"2019-05-18T18:20:42+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:442"},{"text":"//StringIndexer","user":"anonymous","dateUpdated":"2019-05-18T18:23:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558196441888_360321639","id":"20190518-182041_1958510350","dateCreated":"2019-05-18T18:20:41+0200","dateStarted":"2019-05-18T18:23:31+0200","dateFinished":"2019-05-18T18:23:31+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:443"},{"text":"import org.apache.spark.ml.feature.StringIndexer\n\nval df = spark.createDataFrame(\n  Seq((0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"))\n).toDF(\"id\", \"category\")\n\nval indexer = new StringIndexer()\n  .setInputCol(\"category\")\n  .setOutputCol(\"categoryIndex\")\n\nval indexed = indexer.fit(df).transform(df)\nindexed.show()","user":"anonymous","dateUpdated":"2019-05-18T18:23:33+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+-------------+\n| id|category|categoryIndex|\n+---+--------+-------------+\n|  0|       a|          0.0|\n|  1|       b|          2.0|\n|  2|       c|          1.0|\n|  3|       a|          0.0|\n|  4|       a|          0.0|\n|  5|       c|          1.0|\n+---+--------+-------------+\n\nimport org.apache.spark.ml.feature.StringIndexer\ndf: org.apache.spark.sql.DataFrame = [id: int, category: string]\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_53b04d4372c9\nindexed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1558196577964_95216398","id":"20190518-182257_2030018321","dateCreated":"2019-05-18T18:22:57+0200","dateStarted":"2019-05-18T18:23:33+0200","dateFinished":"2019-05-18T18:23:34+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:444"},{"text":"//VectorAssembler","user":"anonymous","dateUpdated":"2019-05-18T18:26:11+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558196613817_-375865598","id":"20190518-182333_226267770","dateCreated":"2019-05-18T18:23:33+0200","dateStarted":"2019-05-18T18:26:11+0200","dateFinished":"2019-05-18T18:26:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:445"},{"text":"import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n\nval dataset = spark.createDataFrame(\n  Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))\n).toDF(\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\")\n\nval assembler = new VectorAssembler()\n  .setInputCols(Array(\"hour\", \"mobile\", \"userFeatures\"))\n  .setOutputCol(\"features\")\n\nval output = assembler.transform(dataset)\nprintln(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\noutput.select(\"features\", \"clicked\").show(false)","user":"anonymous","dateUpdated":"2019-05-18T18:26:13+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n+-----------------------+-------+\n|features               |clicked|\n+-----------------------+-------+\n|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n+-----------------------+-------+\n\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\ndataset: org.apache.spark.sql.DataFrame = [id: int, hour: int ... 3 more fields]\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_cb97e5e63812\noutput: org.apache.spark.sql.DataFrame = [id: int, hour: int ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1558196765373_-1355510793","id":"20190518-182605_2031046161","dateCreated":"2019-05-18T18:26:05+0200","dateStarted":"2019-05-18T18:26:13+0200","dateFinished":"2019-05-18T18:26:13+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:446"},{"text":"//Normalizer","user":"anonymous","dateUpdated":"2019-05-18T18:27:45+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558196773039_-373562444","id":"20190518-182613_1927523483","dateCreated":"2019-05-18T18:26:13+0200","dateStarted":"2019-05-18T18:27:45+0200","dateFinished":"2019-05-18T18:27:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:447"},{"text":"import org.apache.spark.ml.feature.Normalizer\nimport org.apache.spark.ml.linalg.Vectors\n\nval dataFrame = spark.createDataFrame(Seq(\n  (0, Vectors.dense(1.0, 0.5, -1.0)),\n  (1, Vectors.dense(2.0, 1.0, 1.0)),\n  (2, Vectors.dense(4.0, 10.0, 2.0))\n)).toDF(\"id\", \"features\")\n\n// Normalize each Vector using $L^1$ norm.\nval normalizer = new Normalizer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"normFeatures\")\n  .setP(1.0)\n\nval l1NormData = normalizer.transform(dataFrame)\nprintln(\"Normalized using L^1 norm\")\nl1NormData.show()\n\n// Normalize each Vector using $L^\\infty$ norm.\nval lInfNormData = normalizer.transform(dataFrame, normalizer.p -> Double.PositiveInfinity)\nprintln(\"Normalized using L^inf norm\")\nlInfNormData.show()","user":"anonymous","dateUpdated":"2019-05-18T18:27:47+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Normalized using L^1 norm\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\nNormalized using L^inf norm\n+---+--------------+--------------+\n| id|      features|  normFeatures|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\nimport org.apache.spark.ml.feature.Normalizer\nimport org.apache.spark.ml.linalg.Vectors\ndataFrame: org.apache.spark.sql.DataFrame = [id: int, features: vector]\nnormalizer: org.apache.spark.ml.feature.Normalizer = normalizer_7dbd9ce535c9\nl1NormData: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 1 more field]\nlInfNormData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, features: vector ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1558196859948_-1187002951","id":"20190518-182739_874280719","dateCreated":"2019-05-18T18:27:39+0200","dateStarted":"2019-05-18T18:27:47+0200","dateFinished":"2019-05-18T18:27:47+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:448"},{"text":"//ChiSqSelector","user":"anonymous","dateUpdated":"2019-05-18T18:30:13+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558196867584_372706889","id":"20190518-182747_1610657315","dateCreated":"2019-05-18T18:27:47+0200","dateStarted":"2019-05-18T18:30:13+0200","dateFinished":"2019-05-18T18:30:13+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:449"},{"text":"import org.apache.spark.ml.feature.ChiSqSelector\nimport org.apache.spark.ml.linalg.Vectors\n\nval data = Seq(\n  (7, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0),\n  (8, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0),\n  (9, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0)\n)\n\nval df = spark.createDataset(data).toDF(\"id\", \"features\", \"clicked\")\n\nval selector = new ChiSqSelector()\n  .setNumTopFeatures(1)\n  .setFeaturesCol(\"features\")\n  .setLabelCol(\"clicked\")\n  .setOutputCol(\"selectedFeatures\")\n\nval result = selector.fit(df).transform(df)\n\nprintln(s\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\")\nresult.show()","user":"anonymous","dateUpdated":"2019-05-18T18:30:08+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ChiSqSelector output with top 1 features selected\n+---+------------------+-------+----------------+\n| id|          features|clicked|selectedFeatures|\n+---+------------------+-------+----------------+\n|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n+---+------------------+-------+----------------+\n\nimport org.apache.spark.ml.feature.ChiSqSelector\nimport org.apache.spark.ml.linalg.Vectors\ndata: Seq[(Int, org.apache.spark.ml.linalg.Vector, Double)] = List((7,[0.0,0.0,18.0,1.0],1.0), (8,[0.0,1.0,12.0,0.0],0.0), (9,[1.0,0.0,15.0,0.1],0.0))\ndf: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 1 more field]\nselector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_5c83185208aa\nresult: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1558197005910_-108920691","id":"20190518-183005_1332033141","dateCreated":"2019-05-18T18:30:05+0200","dateStarted":"2019-05-18T18:30:08+0200","dateFinished":"2019-05-18T18:30:08+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:450"},{"text":"//                                                          MODELOS DESCRIPTIVOS","user":"anonymous","dateUpdated":"2019-05-18T18:43:52+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558197008285_1449739982","id":"20190518-183008_1952024331","dateCreated":"2019-05-18T18:30:08+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:451"},{"text":"// K-means clustering\n// •\n//  Es uno de los algoritmos de clustering más usados que agrupa\n// los datos en un número predefinido de clusters.\n// •\n//  Está implementado como un “Estimator” y genera como modelo\n// base un KMeansModel.\n","user":"anonymous","dateUpdated":"2019-05-19T18:01:29+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558281598411_646304905","id":"20190519-175958_233476378","dateCreated":"2019-05-19T17:59:58+0200","dateStarted":"2019-05-19T18:01:29+0200","dateFinished":"2019-05-19T18:01:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:452"},{"text":"import org.apache.spark.ml.clustering.KMeans\n// Loads data.\nval dataset = spark.read.format(\"libsvm\").load(\"/home/isa/Escritorio/Universidad/master/BigData/Modulo_2/Bloque2.2.2-SparkMLib/sample_kmeans_data.txt\")\n// Trains a k-means model.\nval kmeans = new KMeans().setK(2).setSeed(1L)\nval model = kmeans.fit(dataset)\n// Evaluate clustering by computing Within Set Sum of Squared\nval WSSSE = model.computeCost(dataset)\n\nprintln(s\"Within Set Sum of Squared Errors = $WSSSE\")\n// Shows the result.\nprintln(\"Cluster Centers: \")\nmodel.clusterCenters.foreach(println)\n\nmodel.clusterCenters.foreach(println)","user":"anonymous","dateUpdated":"2019-05-19T18:15:51+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Within Set Sum of Squared Errors = 0.11999999999994547\nCluster Centers: \n[0.1,0.1,0.1]\n[9.1,9.1,9.1]\n[0.1,0.1,0.1]\n[9.1,9.1,9.1]\nimport org.apache.spark.ml.clustering.KMeans\ndataset: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nkmeans: org.apache.spark.ml.clustering.KMeans = kmeans_37eaf4564494\nmodel: org.apache.spark.ml.clustering.KMeansModel = kmeans_37eaf4564494\nWSSSE: Double = 0.11999999999994547\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.43:4040/jobs/job?id=35","http://192.168.1.43:4040/jobs/job?id=36","http://192.168.1.43:4040/jobs/job?id=37","http://192.168.1.43:4040/jobs/job?id=38","http://192.168.1.43:4040/jobs/job?id=39","http://192.168.1.43:4040/jobs/job?id=40","http://192.168.1.43:4040/jobs/job?id=41","http://192.168.1.43:4040/jobs/job?id=42","http://192.168.1.43:4040/jobs/job?id=43","http://192.168.1.43:4040/jobs/job?id=44","http://192.168.1.43:4040/jobs/job?id=45"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558281689307_1419246050","id":"20190519-180129_892277176","dateCreated":"2019-05-19T18:01:29+0200","dateStarted":"2019-05-19T18:15:51+0200","dateFinished":"2019-05-19T18:15:52+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:453"},{"text":"// FP-growth1\n// •\n//  PFP 2 : versión paralela en MLlib\n// •\n//  Calcula todos los conjuntos de elementos frecuentes de un\n// conjunto de datos mediante la creación de una estructura\n// de datos FP-Tree sobre las transacciones de la base de\n// datos.\n// •\n//  PFP calcula los árboles FP-Tree de forma distribuida\n// siendo más escalable.\n","user":"anonymous","dateUpdated":"2019-05-19T18:20:43+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558282262202_-124657591","id":"20190519-181102_286331865","dateCreated":"2019-05-19T18:11:02+0200","dateStarted":"2019-05-19T18:20:43+0200","dateFinished":"2019-05-19T18:20:43+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:454"},{"text":"import org.apache.spark.ml.fpm.FPGrowth\n\nval dataset = spark.createDataset(\nSeq( \"1 2 5\", \"1 2 3 5\", \"1 2\") ).map(t => t.split(\"\")).toDF(\"items\")\nval fpgrowth = new FPGrowth().setItemsCol(\"items\").setMinSupport(0.5).setMinConfidence(0.6)\nval model = fpgrowth.fit(dataset)\n// Display frequent itemsets.\nmodel.freqItemsets.show()\n// Display generated association rules.\nmodel.associationRules.show()\n// transform examines the input items against all the association rules and summarize the consequents as prediction\nmodel.transform(dataset).show()\n\n","user":"anonymous","dateUpdated":"2019-05-19T18:21:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 74.0 failed 1 times, most recent failure: Lost task 0.0 in stage 74.0 (TID 74, localhost, executor driver): org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(1,  , 2,  , 5).\n\tat org.apache.spark.mllib.fpm.FPGrowth$$anonfun$5.apply(FPGrowth.scala:243)\n\tat org.apache.spark.mllib.fpm.FPGrowth$$anonfun$5.apply(FPGrowth.scala:240)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n  at org.apache.spark.mllib.fpm.FPGrowth.genFreqItems(FPGrowth.scala:249)\n  at org.apache.spark.mllib.fpm.FPGrowth.run(FPGrowth.scala:216)\n  at org.apache.spark.ml.fpm.FPGrowth.genericFit(FPGrowth.scala:167)\n  at org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:157)\n  ... 47 elided\nCaused by: org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(1,  , 2,  , 5).\n  at org.apache.spark.mllib.fpm.FPGrowth$$anonfun$5.apply(FPGrowth.scala:243)\n  at org.apache.spark.mllib.fpm.FPGrowth$$anonfun$5.apply(FPGrowth.scala:240)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n  at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:108)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n  ... 3 more\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.43:4040/jobs/job?id=46","http://192.168.1.43:4040/jobs/job?id=47"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558282843332_-21858880","id":"20190519-182043_1891743883","dateCreated":"2019-05-19T18:20:43+0200","dateStarted":"2019-05-19T18:21:31+0200","dateFinished":"2019-05-19T18:21:32+0200","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:455"},{"text":"//Decision tree","user":"anonymous","dateUpdated":"2019-05-19T18:27:48+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558282868448_-399564642","id":"20190519-182108_795759770","dateCreated":"2019-05-19T18:21:08+0200","dateStarted":"2019-05-19T18:27:48+0200","dateFinished":"2019-05-19T18:27:48+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:456"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n\n// Load the data stored in LIBSVM format as a DataFrame.\nval data = spark.read.format(\"libsvm\").load(\"/home/isa/Escritorio/Universidad/master/BigData/Modulo_2/Bloque2.2.2-SparkMLib/sample_libsvm_data.txt\")\n\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval labelIndexer = new StringIndexer()\n  .setInputCol(\"label\")\n  .setOutputCol(\"indexedLabel\")\n  .fit(data)\n// Automatically identify categorical features, and index them.\nval featureIndexer = new VectorIndexer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"indexedFeatures\")\n  .setMaxCategories(4) // features with > 4 distinct values are treated as continuous.\n  .fit(data)\n\n// Split the data into training and test sets (30% held out for testing).\nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n\n// Train a DecisionTree model.\nval dt = new DecisionTreeClassifier()\n  .setLabelCol(\"indexedLabel\")\n  .setFeaturesCol(\"indexedFeatures\")\n\n// Convert indexed labels back to original labels.\nval labelConverter = new IndexToString()\n  .setInputCol(\"prediction\")\n  .setOutputCol(\"predictedLabel\")\n  .setLabels(labelIndexer.labels)\n\n// Chain indexers and tree in a Pipeline.\nval pipeline = new Pipeline()\n  .setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))\n\n// Train model. This also runs the indexers.\nval model = pipeline.fit(trainingData)\n\n// Make predictions.\nval predictions = model.transform(testData)\n\n// Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n// Select (prediction, true label) and compute test error.\nval evaluator = new MulticlassClassificationEvaluator()\n  .setLabelCol(\"indexedLabel\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"accuracy\")\nval accuracy = evaluator.evaluate(predictions)\nprintln(s\"Test Error = ${(1.0 - accuracy)}\")\n\nval treeModel = model.stages(2).asInstanceOf[DecisionTreeClassificationModel]\nprintln(s\"Learned classification tree model:\\n ${treeModel.toDebugString}\")","user":"anonymous","dateUpdated":"2019-05-19T18:31:24+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+-----+--------------------+\n|predictedLabel|label|            features|\n+--------------+-----+--------------------+\n|           0.0|  0.0|(692,[98,99,100,1...|\n|           0.0|  0.0|(692,[123,124,125...|\n|           0.0|  0.0|(692,[124,125,126...|\n|           0.0|  0.0|(692,[124,125,126...|\n|           0.0|  0.0|(692,[124,125,126...|\n+--------------+-----+--------------------+\nonly showing top 5 rows\n\nTest Error = 0.05882352941176472\nLearned classification tree model:\n DecisionTreeClassificationModel (uid=dtc_e5dee2615e47) of depth 1 with 3 nodes\n  If (feature 406 <= 20.0)\n   Predict: 1.0\n  Else (feature 406 > 20.0)\n   Predict: 0.0\n\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nlabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_d9c581f6101a\nfeatureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_7f0a92e6a7b8\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ndt: org.apache.spark.m..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.43:4040/jobs/job?id=61","http://192.168.1.43:4040/jobs/job?id=62","http://192.168.1.43:4040/jobs/job?id=63","http://192.168.1.43:4040/jobs/job?id=64","http://192.168.1.43:4040/jobs/job?id=65","http://192.168.1.43:4040/jobs/job?id=66","http://192.168.1.43:4040/jobs/job?id=67","http://192.168.1.43:4040/jobs/job?id=68","http://192.168.1.43:4040/jobs/job?id=69","http://192.168.1.43:4040/jobs/job?id=70","http://192.168.1.43:4040/jobs/job?id=71"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558283268594_-1658534354","id":"20190519-182748_1102049336","dateCreated":"2019-05-19T18:27:48+0200","dateStarted":"2019-05-19T18:31:24+0200","dateFinished":"2019-05-19T18:31:26+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:457"},{"text":"//Random forest","user":"anonymous","dateUpdated":"2019-05-19T18:34:23+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558283484734_839552502","id":"20190519-183124_564239374","dateCreated":"2019-05-19T18:31:24+0200","dateStarted":"2019-05-19T18:34:23+0200","dateFinished":"2019-05-19T18:34:24+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:458"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n\n// Load and parse the data file, converting it to a DataFrame.\nval data = spark.read.format(\"libsvm\").load(\"/home/isa/Escritorio/Universidad/master/BigData/Modulo_2/Bloque2.2.2-SparkMLib/sample_libsvm_data.txt\")\n\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval labelIndexer = new StringIndexer()\n  .setInputCol(\"label\")\n  .setOutputCol(\"indexedLabel\")\n  .fit(data)\n// Automatically identify categorical features, and index them.\n// Set maxCategories so features with > 4 distinct values are treated as continuous.\nval featureIndexer = new VectorIndexer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"indexedFeatures\")\n  .setMaxCategories(4)\n  .fit(data)\n\n// Split the data into training and test sets (30% held out for testing).\nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n\n// Train a RandomForest model.\nval rf = new RandomForestClassifier()\n  .setLabelCol(\"indexedLabel\")\n  .setFeaturesCol(\"indexedFeatures\")\n  .setNumTrees(10)\n\n// Convert indexed labels back to original labels.\nval labelConverter = new IndexToString()\n  .setInputCol(\"prediction\")\n  .setOutputCol(\"predictedLabel\")\n  .setLabels(labelIndexer.labels)\n\n// Chain indexers and forest in a Pipeline.\nval pipeline = new Pipeline()\n  .setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))\n\n// Train model. This also runs the indexers.\nval model = pipeline.fit(trainingData)\n\n// Make predictions.\nval predictions = model.transform(testData)\n\n// Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n// Select (prediction, true label) and compute test error.\nval evaluator = new MulticlassClassificationEvaluator()\n  .setLabelCol(\"indexedLabel\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"accuracy\")\nval accuracy = evaluator.evaluate(predictions)\nprintln(s\"Test Error = ${(1.0 - accuracy)}\")\n\nval rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]\nprintln(s\"Learned classification forest model:\\n ${rfModel.toDebugString}\")","user":"anonymous","dateUpdated":"2019-05-19T18:34:27+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+-----+--------------------+\n|predictedLabel|label|            features|\n+--------------+-----+--------------------+\n|           0.0|  0.0|(692,[124,125,126...|\n|           0.0|  0.0|(692,[124,125,126...|\n|           0.0|  0.0|(692,[124,125,126...|\n|           0.0|  0.0|(692,[125,126,127...|\n|           0.0|  0.0|(692,[126,127,128...|\n+--------------+-----+--------------------+\nonly showing top 5 rows\n\nTest Error = 0.03703703703703709\nLearned classification forest model:\n RandomForestClassificationModel (uid=rfc_59da5872f8e4) with 10 trees\n  Tree 0 (weight 1.0):\n    If (feature 552 <= 0.0)\n     If (feature 497 <= 0.0)\n      If (feature 490 <= 0.0)\n       Predict: 1.0\n      Else (feature 490 > 0.0)\n       Predict: 0.0\n     Else (feature 497 > 0.0)\n      Predict: 1.0\n    Else (feature 552 > 0.0)\n     If (feature 598 <= 126.0)\n      If (feature 355 <= 0.0)\n       Predict: 0.0\n      Else (feature 355 > 0.0)\n       Predict: 1.0\n     Else (feature 598 > 126.0)\n      Predict: 1.0\n  Tree 1 (weight 1.0):\n    If (feature 463 <= 0.0)\n     If (feature 317 <= 0.0)\n      If (feature 600 <= 195.0)\n       Predict: 0.0\n      Else (feature 600 > 195.0)\n       Predict: 1.0\n     Else (feature 317 > 0.0)\n      Predict: 1.0\n    Else (feature 463 > 0.0)\n     Predict: 0.0\n  Tree 2 (weight 1.0):\n    If (feature 540 <= 65.0)\n     If (feature 235 <= 101.0)\n      Predict: 0.0\n     Else (feature 235 > 101.0)\n      Predict: 1.0\n    Else (feature 540 > 65.0)\n     Predict: 1.0\n  Tree 3 (weight 1.0):\n    If (feature 328 <= 0.0)\n     If (feature 341 <= 6.0)\n      If (feature 550 <= 43.0)\n       Predict: 0.0\n      Else (feature 550 > 43.0)\n       Predict: 1.0\n     Else (feature 341 > 6.0)\n      Predict: 1.0\n    Else (feature 328 > 0.0)\n     Predict: 1.0\n  Tree 4 (weight 1.0):\n    If (feature 524 <= 0.0)\n     If (feature 407 <= 0.0)\n      Predict: 1.0\n     Else (feature 407 > 0.0)\n      Predict: 0.0\n    Else (feature 524 > 0.0)\n     Predict: 1.0\n  Tree 5 (weight 1.0):\n    If (feature 462 <= 0.0)\n     Predict: 1.0\n    Else (feature 462 > 0.0)\n     Predict: 0.0\n  Tree 6 (weight 1.0):\n    If (feature 385 <= 0.0)\n     If (feature 663 <= 41.0)\n      If (feature 346 <= 4.0)\n       Predict: 0.0\n      Else (feature 346 > 4.0)\n       Predict: 1.0\n     Else (feature 663 > 41.0)\n      Predict: 1.0\n    Else (feature 385 > 0.0)\n     Predict: 1.0\n  Tree 7 (weight 1.0):\n    If (feature 512 <= 0.0)\n     If (feature 288 <= 145.0)\n      Predict: 0.0\n     Else (feature 288 > 145.0)\n      Predict: 1.0\n    Else (feature 512 > 0.0)\n     Predict: 1.0\n  Tree 8 (weight 1.0):\n    If (feature 462 <= 0.0)\n     Predict: 1.0\n    Else (feature 462 > 0.0)\n     Predict: 0.0\n  Tree 9 (weight 1.0):\n    If (feature 377 <= 45.0)\n     If (feature 435 <= 0.0)\n      Predict: 1.0\n     Else (feature 435 > 0.0)\n      Predict: 0.0\n    Else (feature 377 > 45.0)\n     If (feature 317 <= 0.0)\n      Predict: 0.0\n     Else (feature 317 > 0.0)\n      Predict: 1.0\n\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nlabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_710190f761db\nfeatureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_273eda09dd00\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\nrf: org.apache.spark.ml.classification.RandomForestClassifier..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.1.43:4040/jobs/job?id=86","http://192.168.1.43:4040/jobs/job?id=87","http://192.168.1.43:4040/jobs/job?id=88","http://192.168.1.43:4040/jobs/job?id=89","http://192.168.1.43:4040/jobs/job?id=90","http://192.168.1.43:4040/jobs/job?id=91","http://192.168.1.43:4040/jobs/job?id=92","http://192.168.1.43:4040/jobs/job?id=93","http://192.168.1.43:4040/jobs/job?id=94","http://192.168.1.43:4040/jobs/job?id=95","http://192.168.1.43:4040/jobs/job?id=96","http://192.168.1.43:4040/jobs/job?id=97","http://192.168.1.43:4040/jobs/job?id=98","http://192.168.1.43:4040/jobs/job?id=99"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558283663821_644461843","id":"20190519-183423_1744118865","dateCreated":"2019-05-19T18:34:23+0200","dateStarted":"2019-05-19T18:34:27+0200","dateFinished":"2019-05-19T18:34:30+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:459"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558283667051_-1068391089","id":"20190519-183427_1456628254","dateCreated":"2019-05-19T18:34:27+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:460"}],"name":"MLlib_Activities","id":"2EE6CAPYS","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}